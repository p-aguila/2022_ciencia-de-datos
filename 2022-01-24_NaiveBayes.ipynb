{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6635fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-01-24T15:30:02.706525-03:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed749cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descripcion</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aunque parezca mentira, las emisiones de dióxi...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hubo un proyecto impulsado por la Unión Europe...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China ha confirmado la conclusión con éxito de...</td>\n",
       "      <td>tecnología</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En su fructífera carrera como humorista, actor...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tras dos años de negociación entre la instituc...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         descripcion   categoria\n",
       "0  Aunque parezca mentira, las emisiones de dióxi...     cultura\n",
       "1  Hubo un proyecto impulsado por la Unión Europe...     cultura\n",
       "2  China ha confirmado la conclusión con éxito de...  tecnología\n",
       "3  En su fructífera carrera como humorista, actor...     cultura\n",
       "4  Tras dos años de negociación entre la instituc...     cultura"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "noticias = pd.read_csv('datasets/noticias.csv')\n",
    "noticias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a1e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16495, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fd6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cultura       9001\n",
       "tecnología    4198\n",
       "ocio          3296\n",
       "Name: categoria, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias.categoria.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf855f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construye un vector por cada descripcion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9612138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict de stopwords\n",
    "# https://github.com/stopwords-iso/stopwords-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41e3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open ('datasets/stopwords_es.json') as fname:\n",
    "    stopwords_es = json.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf88b497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f21e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construccion del vectorizador\n",
    "vectorizador = TfidfVectorizer(strip_accents='unicode', stop_words=stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef994ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "                            '_', 'a', 'actualmente', 'acuerdo', 'adelante',\n",
       "                            'ademas', 'ademÃ¡s', 'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                            'ahi', 'ahora', 'ahÃ\\xad', 'al', 'algo', 'alguna',\n",
       "                            'algunas', 'alguno', 'algunos', 'algÃºn', ...],\n",
       "                strip_accents='unicode')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591ab8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# resultado de la vectorizacion\n",
    "ms = vectorizador.fit_transform(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81039f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16495, 59952)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crea una matriz con mismo num de filas, \n",
    "# pero un gran num de columnas, que representan a cada palabra\n",
    "ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867e421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16495, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cda0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25003)\t0.20646849949017032\n",
      "  (0, 42353)\t0.12098270076249901\n",
      "  (0, 7113)\t0.18617962900617258\n",
      "  (0, 23697)\t0.20152756703885846\n",
      "  (0, 20224)\t0.21283846931034292\n",
      "  (0, 39408)\t0.1488387187872587\n",
      "  (0, 46342)\t0.11200473254998575\n",
      "  (0, 31946)\t0.11714075343588001\n",
      "  (0, 23417)\t0.10305267703099465\n",
      "  (0, 13579)\t0.1549369993817717\n",
      "  (0, 45450)\t0.12242728210819159\n",
      "  (0, 9850)\t0.3335893104245706\n",
      "  (0, 22173)\t0.22354574580342873\n",
      "  (0, 21196)\t0.21283846931034292\n",
      "  (0, 50942)\t0.20152756703885846\n",
      "  (0, 55109)\t0.1667946552122853\n",
      "  (0, 20240)\t0.17443024915712568\n",
      "  (0, 1731)\t0.19407726617604626\n",
      "  (0, 4575)\t0.06378000565191391\n",
      "  (0, 812)\t0.11255537108035345\n",
      "  (0, 58023)\t0.14309251962667677\n",
      "  (0, 55338)\t0.20378201599818133\n",
      "  (0, 25006)\t0.20152756703885846\n",
      "  (0, 2208)\t0.20646849949017032\n",
      "  (0, 6534)\t0.20646849949017032\n",
      "  :\t:\n",
      "  (16494, 49162)\t0.22045065666541835\n",
      "  (16494, 12094)\t0.22045065666541835\n",
      "  (16494, 28660)\t0.1727600813376245\n",
      "  (16494, 11971)\t0.18682121480155262\n",
      "  (16494, 15768)\t0.1820591474058168\n",
      "  (16494, 50342)\t0.1610446436100992\n",
      "  (16494, 5895)\t0.16764243598450485\n",
      "  (16494, 34989)\t0.1578392986618508\n",
      "  (16494, 40042)\t0.14821023670121478\n",
      "  (16494, 8892)\t0.19795600584841475\n",
      "  (16494, 47699)\t0.16841994497983445\n",
      "  (16494, 55958)\t0.18865693978022244\n",
      "  (16494, 20239)\t0.17475562800028605\n",
      "  (16494, 58766)\t0.16222102037798086\n",
      "  (16494, 21694)\t0.1301275042062057\n",
      "  (16494, 24254)\t0.15592699825697956\n",
      "  (16494, 12056)\t0.1727600813376245\n",
      "  (16494, 14944)\t0.16841994497983445\n",
      "  (16494, 13920)\t0.2087352189378931\n",
      "  (16494, 8954)\t0.31469213698851295\n",
      "  (16494, 22763)\t0.11649857471156681\n",
      "  (16494, 36206)\t0.15686322289502652\n",
      "  (16494, 40121)\t0.24327355948459792\n",
      "  (16494, 58023)\t0.29642047340242955\n",
      "  (16494, 55338)\t0.105535149235399\n"
     ]
    }
   ],
   "source": [
    "# se interpreta de la siguiente forma: en la fila 0, columna 25003, se encuentra el valor 0.2064...\n",
    "# es una matriz sparse (dispersa)\n",
    "print(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c104db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras fuentes de stopwords\n",
    "#import nltk \n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords \n",
    "#sw_nltk = stopwords.words('spanish') \n",
    "#print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac30d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasbt.github.io/mlxtend\n",
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77329afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un pipeline para encapsular pasos\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# de disperso a denso\n",
    "from mlxtend.preprocessing import DenseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcebde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f692018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un pipeline basado en NB Gauss:\n",
    "# entregar el vectorizador (no el 'ms' pq es solo una matriz con numeros)\n",
    "# genera una matriz sparse con los ceros que faltan\n",
    "# el GaussianNB lo toma y genera los pronosticos\n",
    "pipeline_gauss = make_pipeline(vectorizador, DenseTransformer(), GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "706c947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6',\n",
       "                                             '7', '8', '9', '_', 'a',\n",
       "                                             'actualmente', 'acuerdo',\n",
       "                                             'adelante', 'ademas', 'ademÃ¡s',\n",
       "                                             'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                                             'ahi', 'ahora', 'ahÃ\\xad', 'al',\n",
       "                                             'algo', 'alguna', 'algunas',\n",
       "                                             'alguno', 'algunos', 'algÃºn', ...],\n",
       "                                 strip_accents='unicode')),\n",
       "                ('densetransformer', DenseTransformer()),\n",
       "                ('gaussiannb', GaussianNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63fe4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir en conjunto de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(noticias.descripcion, \n",
    "                                                    noticias.categoria, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525e8d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6',\n",
       "                                             '7', '8', '9', '_', 'a',\n",
       "                                             'actualmente', 'acuerdo',\n",
       "                                             'adelante', 'ademas', 'ademÃ¡s',\n",
       "                                             'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                                             'ahi', 'ahora', 'ahÃ\\xad', 'al',\n",
       "                                             'algo', 'alguna', 'algunas',\n",
       "                                             'alguno', 'algunos', 'algÃºn', ...],\n",
       "                                 strip_accents='unicode')),\n",
       "                ('densetransformer', DenseTransformer()),\n",
       "                ('gaussiannb', GaussianNB())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ahora pedimos que el pipeline haga la transformacion\n",
    "# entrenamiento\n",
    "pipeline_gauss.fit(X=noticias.descripcion, y=noticias.categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f5b75e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cultura', 'cultura', 'tecnología', ..., 'cultura', 'tecnología',\n",
       "       'ocio'], dtype='<U10')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# realizar la prediccion\n",
    "pipeline_gauss.predict(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc0a5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# f-1 score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffa6340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_curso(modelo, X, y):\n",
    "    preds = modelo.predict(X)\n",
    "    return f1_score(y, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "058fc207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.62988784, 0.64352834, 0.64807517, 0.63776902, 0.63504092])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_gauss, \n",
    "                noticias.descripcion, \n",
    "                noticias.categoria, \n",
    "                scoring=f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1658ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construccion del vectorizador\n",
    "# max_features: limitar cantidad de columnas\n",
    "vectorizador = TfidfVectorizer(strip_accents='unicode', stop_words=stopwords_es, max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "569fd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bernoulli pipeline\n",
    "pipeline_bernoulii = make_pipeline(vectorizador, DenseTransformer(), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "621c3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.66595938, 0.66110943, 0.66414065, 0.66565626, 0.65807821])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_bernoulii, \n",
    "                noticias.descripcion, \n",
    "                noticias.categoria, \n",
    "                scoring=f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2837225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nultinomial pipeline\n",
    "pipeline_multinomial = make_pipeline(vectorizador, DenseTransformer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79e33b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patri\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68778418, 0.67626554, 0.67262807, 0.68475296, 0.66717187])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_multinomial, \n",
    "                noticias.descripcion, \n",
    "                noticias.categoria, \n",
    "                scoring=f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da673e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# los resultados anteriores indican que nuestro modelo acierta aprox el 66% de las veces.\n",
    "# Aun se puede seguir mejorando"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
